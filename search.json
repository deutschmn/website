[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Patrick, and I’m a Machine Learning Engineer at Dedalus HealthCare where I’m building models for clinical risk prediction and NLP for health."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello! This is work in progress.",
    "section": "",
    "text": "More is More: An Analysis of Using Efficient Transformers for Fact-Checking\n\n\n\n\n\n\n\nnlp\n\n\nml\n\n\nresearch\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2022\n\n\nPatrick Deutschmann\n\n\n\n\n\n\n  \n\n\n\n\nSecurity in Container Orchestration\n\n\n\n\n\n\n\nresearch\n\n\nops\n\n\nsecurity\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2019\n\n\nPatrick Deutschmann\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/security-in-container-orchestration/index.html",
    "href": "blog/security-in-container-orchestration/index.html",
    "title": "Security in Container Orchestration",
    "section": "",
    "text": "I wrote my bachelor’s thesis on the issue of security in container orchestration, specifically in Kubernetes:\n\nContainerisation is increasingly gaining traction to run modern applications in distributed environments. To run containers on a large scale and with high availability, container orchestration systems are commonly employed. The most widely used container orchestration system today is Kubernetes, which is highly ﬂexible, but also comes with signiﬁcant complexity.\n\n\nIn this thesis, we analyse the security of Kubernetes architectures. To do so, we create a layer model to give a holistic view of all relevant aspects. We demonstrate how an example application can securely run in a Kubernetes cluster and which conﬁgurations are necessary to strengthen security by employing multiple redundant barriers.\n\n\nOur research shows that most Kubernetes installers already come with reasonably secure default conﬁgurations. However, custom adaptations in consideration of the deployed applications and their requirements to the runtime environment are imperative for secure cluster setup.\n\nIn short, I tried to get a holistic view of the relevant security aspects of container orchestration in Kubernetes and categorised them into a layer model.\n\n\n\nMy layer architecture for categorising Kubernetes security aspects\n\n\nI demonstrated my model with a sample architecture run on Google Kubernetes Engine. At the time of this writing, most Kubernetes installers already come with relatively secure default setups. However, there are still plenty of pitfalls and things to look out for in order not to end up in a bad place.\nIf you’re interested, you can read my full thesis here."
  },
  {
    "objectID": "blog/more-is-more/index.html",
    "href": "blog/more-is-more/index.html",
    "title": "More is More: An Analysis of Using Efficient Transformers for Fact-Checking",
    "section": "",
    "text": "For my Master’s thesis, I worked on the NLP task of fact-checking. It started out with the observation that current, traditional Transformers (such as BERT) can only handle a limited amount of evidence. That is because they have a quadratic memory complexity in the sequence length. Therefore, processing more evidence becomes quadratically more expensive with every token added.\nThe idea of my thesis is to analyse the effect of using more efficient Transformer models with sub-quadratic complexity to increase the amount of evidence they can process. Doing so increased prediction accuracy for long documents and reduced computational costs.\nIn this blog post, I will give an overview of my research. If you want to get a complete picture, have a look at my full thesis.\nWith this thesis, I have obtained my Master’s degree in computer science at Graz University of Technology. I worked on the project while employed at Buster.Ai, a Paris-based startup with a focus on NLP. I would like to thank the whole team once again for their support and the computational resources I was provided with."
  },
  {
    "objectID": "blog/more-is-more/index.html#what-is-fact-checking-why-should-i-care",
    "href": "blog/more-is-more/index.html#what-is-fact-checking-why-should-i-care",
    "title": "More is More: An Analysis of Using Efficient Transformers for Fact-Checking",
    "section": "What is fact-checking? Why should I care?",
    "text": "What is fact-checking? Why should I care?\nFalse information on and off the web is becoming increasingly common. Economic fallout, societal conflicts and health risks follow. Automatic fact-checking systems are one way to combat this ever more dangerous problem. In principle, they work like this: A claim that is to be verified is input into the system. It, then, predicts a verdict. This verdict is not (and cannot be) whether the claim is true or false since this would require the model to make definitive statements about the world. As this is infeasible, such systems resort to predicting whether a claim is supported or refuted by their evidence base (a.k.a. knowledge base). For example, for the famous fact-checking data sets FEVER and FEVEROUS, this evidence base is Wikipedia. Here is an example of such a claim from the FEVEROUS data set:\n\nThe evidence base (Wikipedia) is used to predict the verdict of the claim (Refuted)."
  },
  {
    "objectID": "blog/more-is-more/index.html#how-do-fact-checking-systems-work",
    "href": "blog/more-is-more/index.html#how-do-fact-checking-systems-work",
    "title": "More is More: An Analysis of Using Efficient Transformers for Fact-Checking",
    "section": "How do fact-checking systems work?",
    "text": "How do fact-checking systems work?\nFact-checking systems with explicit knowledge bases work as follows:\n\nA retrieval component extracts the relevant evidence (also called gold evidence) from the evidence base. In the previous example, this would be the Wikipedia article of Micheal McCafferty. Then, an entailment model predicts a verdict, i.e., whether the retrieved evidence supports or refutes the claim. The entailment task is also known as Natural Language Inference (NLI). Most state-of-the-art entailment models today are Transformers, which process one joint input sequence per sample. In the case of fact-checking, this means that the claim and the relevant evidence are concatenated into one long sequence of length \\(N\\).\n\nNaturally, the more evidence the model should process, the longer the input sequence becomes. However, as Transformers have quadratic complexity in the input length, adding more evidence becomes quadratically more expensive.\n\nTherefore, the sequence has to be cut off at some point.\n\n\n\n\n\n\nRoBERTa predictions on FEVER\nCutting off irrelevant evidence is obviously no problem, but cutting off gold evidence is. As you can see in this plot, samples for which the evidence has been cut off are significantly less likely to be correctly classified than samples for which the models have seen the gold evidence."
  },
  {
    "objectID": "blog/more-is-more/index.html#my-work",
    "href": "blog/more-is-more/index.html#my-work",
    "title": "More is More: An Analysis of Using Efficient Transformers for Fact-Checking",
    "section": "My Work",
    "text": "My Work\nThe idea of my thesis is relatively straight-forward:\nUse more efficient Transformers with sub-quadratic complexity that can handle more evidence.\nThe research questions I was aiming to answer were the following:\n\nDoes it improve prediction performance?\nDoes it reduce computational costs?\nWhich model works best?\nIs the approach still interpretable?\n\nFor doing so, I built a complete fact-checking pipeline with preprocessing, retrieval and entailment components. Let’s go through them.\n\nRetrieval\nWhile retrieval methods weren’t the focus of my work, the results of the retrieval step form the foundation of the pipeline. I, therefore, ran some experiments to find the ones who were best suited for my task.\nIn general, there are two broad categories of retrieval methods:\n\nSparse methods: These are based on classical information retrieval techniques and work with term frequency statistics. Famous examples are TF.IDF and BM25.\nDense methods: These approaches use dense neural networks (hence the name). A prominent example is Dense Passage Retrieval (DPR). The principle is relatively simple: All documents are projected into an embedding space, and the embeddings are stored in a database. At retrieval time, an embedding of the claim is computed, and the documents closest to it are returned. Common metrics for closeness are the dot-product and cosine similarity.\n\nIf you’re interested, the creators of the wonderful library haystack have written a more in-depth comparison.\nFor my purposes, I decided to focus on sparse retrieval with BM25 since it yielded sufficient results without further fine-tuning. It was a good baseline to compare entailment methods.\nHowever, this is not to say that I believe sparse methods are sufficient for all fact-checking applications. Their inability to handle synonyms is just the most glaring shortcoming of exact term matching. DPR isn’t the ideal solution either. For example, claims that require multi-hop reasoning (as nicely explained by Ostrowski, Wojciech et al.) require specialised methods, such as Multi-Hop Dense Retrieval (MDR). Due to cascading errors, the shortcomings of retrieval methods cannot entirely be mitigated by using more powerful entailment models. If the relevant evidence hasn’t at all been retrieved, the entailment model won’t be able to make up for that.\n\nNo Passage Retrieval\nMost fact-checking pipelines today use a two-step retrieval process:\n\nFirst, the relevant documents are retrieved and then the relevant passages therein. This second, so-called passage retrieval step is necessary to extract relevant passages from the documents and reduce the amount of information the entailment component needs to handle. However, as I’m using entailment models that can handle much larger input sequences, I can skip the passage retrieval component and simplify the retrieval step to this:\n\n\n\n\n\nThis removes the complexity and computational cost that the passage retrieval incurs. However, it also requires the entailment component to be better able to retrieve the relevant passages in the input sequence."
  },
  {
    "objectID": "blog/more-is-more/index.html#entailment",
    "href": "blog/more-is-more/index.html#entailment",
    "title": "More is More: An Analysis of Using Efficient Transformers for Fact-Checking",
    "section": "Entailment",
    "text": "Entailment\nThe core part of this work concerns the entailment component. The ones in my experiments can handle more evidence by replacing classical Transformer models with more efficient variants.\nAs a baseline, I use RoBERTa, which achieves quite good results for NLI at a moderate number of parameters. However, as it is a traditional quadratic Transformer, it can reasonably only handle up to 512 tokens in most current hardware settings.\nWhile there are a lot of Transformers with sub-quadratic complexity out there, I evaluated the following four, which cover a good mix of different techniques:\nLongformer: This model replaces the full self-attention component with a pattern approach whereby only local tokens, tokens within a dilated sliding window and global tokens can attend to each other. The Hugging Face Blog has a nice writeup if you’re interested in the details. For my experiments, I activated the global attention for all claim tokens. Thereby, the claim could attend to (and be attended to by) all evidence tokens.\nBig Bird: In principle, Big Bird is quite similar to Longformer. There are some important differences, such as random attention and a different global-local attention implementation, which lead to superior results in the original paper on multiple tasks. Hence, I also evaluated this model.\nFNet: In a very different fashion, FNet completely does away with the self-attention component and replaces it with a Fourier transform. The authors argue that this sufficiently mixes the tokens so that the feed-forward layers in the subsequent encoder blocks can learn across hidden and sequence dimension.\nPerceiver IO: This Deepmind model was built to support not only text but also other modalities such as images and audio. Its core idea is to project a large input into a much smaller latent space, on which attention blocks are applied. Therefore, the model’s complexity is no longer quadratic in the input length but in the latent size.\nThis was just a very brief overview of the methods. If you want to learn more, there are plenty of resources on the web. I also provide a more detailed description in my thesis.\n\nExtending Position Embeddings\nLongformer and Big Bird were specifically designed to handle longer input sequences. It is probably therefore that pre-trained checkpoints for 4096 tokens were available (allenai/longformer-base-4096, google/bigbird-roberta-base).\nHowever, for FNet and Perceiver IO, there are no weights available for configurations that can handle as much evidence as the other two. Since I lacked the computational resources to train them from scratch, I experimented with four ways of extending position embeddings: random initialisation, repetition, linear interpolation and nearest-neighbour interpolation.\n\n\n\nIllustration of extending position embeddings: Rows are the hidden dimension (\\(D=3\\)), and columns are the sequence dimension which is being extended from \\(N=3\\) to \\(N'=5\\).\n\n\nI found repetition to train fastest for FNet and nearest-neighbour interpolation for Perceiver IO. However, these are imperfect solutions, and it would likely have improved results to directly pre-train models for longer inputs with masked language modelling (MLM). From a personal learning perspective, it was still interesting to develop solutions to make this work in a resource-constrained setting."
  },
  {
    "objectID": "blog/more-is-more/index.html#evaluation",
    "href": "blog/more-is-more/index.html#evaluation",
    "title": "More is More: An Analysis of Using Efficient Transformers for Fact-Checking",
    "section": "Evaluation",
    "text": "Evaluation\nFinally, we get to the evaluation results. The data sets I used for comparing models and benchmarking overall performance were FEVER, FEVEROUS and FaVIQ. All three have Wikipedia as their evidence bases, but FEVER only contains the introductory paragraphs, while the others contain the full articles.\nTo better analyse how useful efficient Transformers are, I computed where in the input sequence the gold evidence was located for different data sets. After all, using a longer input sequence doesn’t make sense if the gold evidence is found in the beginning already.\n\nAll of these results are based on BM25 retrieval. For FEVER, a lot of gold evidence is before the cutoff line for RoBERTa (512 tokens), while for FEVEROUS, a lot more evidence is after that. To detach the entailment experiments from the retrieval method used, I also generated two synthetic retrieval inputs: In Gold far back, most gold evidence is after what RoBERTa can see, and in Uniform gold, it is uniformly distributed across the 4096 tokens.\nUnsurprisingly, in the gold far back setting, models that can look beyond token 512 have a big advantage and perform considerably better than RoBERTa:\n\nIn the uniform gold setting, which I used to compare all models, I found Longformer to perform best:\n\nIts performance increases when it sees more evidence and clearly beats the RoBERTa baseline. Big Bird develops similarly but consistently worse than Longformer. While Perceiver IO starts relatively promising, I found it to be hard to fine-tune to longer sequences. I suspect that this is because the encoder is hard to adjust to different position embeddings. FNet is outperformed by all other models, which is, however, not too surprising, given that it is also far behind RoBERTa on the GLUE NLI task.\nFinally, I evaluated the best-performing model, Longformer, on FEVER, FaVIQ and FEVEROUS and found that using longer sequences does improve performance:\n\nI also note, however, that on FEVEROUS, performance drops with longer sequence lengths, suggesting that the models are susceptible to noise, i.e., irrelevant evidence. My ablation study on this matter confirmed this hypothesis by showing that RoBERTa’s performance dropped from 95% accuracy when seeing only gold evidence to 86% when irrelevant evidence is appended."
  },
  {
    "objectID": "blog/more-is-more/index.html#key-findings",
    "href": "blog/more-is-more/index.html#key-findings",
    "title": "More is More: An Analysis of Using Efficient Transformers for Fact-Checking",
    "section": "Key Findings",
    "text": "Key Findings\nOverall, I find that feeding more evidence does improve entailment label accuracy. Hence, the title of this thesis is More is More. As for the answers to the research questions (simplified and condensed):\n\nDoes using more efficient Transformers that can handle more evidence improve prediction performance?\n\nYes, for longer input documents.\nYes, if the evidence is only retrieved relatively far back.\nOnly slightly in real-world retrieval results.\n\nDoes it reduce computational costs?\n\nOnly slightly when just swapping out the entailment component, i.e., replacing RoBERTa with Longformer.\nYes, significantly by completely skipping the passage retrieval step, which typically makes up around 35-45% of the inference time.\n\nWhich model works best?\n\nOut of the ones I experimented with, Longformer.\n\nIs the approach still interpretable?\n\nNot out of the box. Due to the removal of the passage retrieval step, this pipeline cannot exhibit which passages it considered for its decision. It can only provide this information on a document level."
  },
  {
    "objectID": "blog/more-is-more/index.html#conclusion",
    "href": "blog/more-is-more/index.html#conclusion",
    "title": "More is More: An Analysis of Using Efficient Transformers for Fact-Checking",
    "section": "Conclusion",
    "text": "Conclusion\nUsing efficient Transformers allowed me to reach 97-99% of the state-of-the-art performance on the FEVER data set at only 40-60% of the inference time. This is made possible by efficient BM25 document retrieval, but primarily because using efficient Transformers allows for completely skipping the passage retrieval step.\nWhile these models obtain impressive results on the data sets they were trained on, plenty of challenges still lie ahead of us. To name a few: Models should become better at handling irrelevant evidence; they should know what they don’t know and be able to explain how they came up with their verdicts.\nIn any case, I enjoyed working on this challenging task and am eager to see how the field develops in the future. I believe that the use of more efficient Transformer models has the potential for unlocking new levels of fact-checking performance with respect to both predictive and computational performance."
  }
]