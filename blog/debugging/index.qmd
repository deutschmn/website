---
title: "Debugging like Hawks and Ants"
author: "Patrick Deutschmann"
date: "2024-07-06"
categories: [ml,dev,debugging]
draft: true
---

# Debugging like Hawks and Ants

Nobody ever explicitly taught me how to debug things. The internet is full of tutorials about programming. Still, there are things you need to pick up as you go. Debugging is one of them. It’s to programming what seasoning is to cooking. Essential, yet hard to formalise. After some time, you develop a feeling for it.

Debugging (and seasoning) are very individual tasks. What you do depends on the situation. Still, there are two patterns I’ve found myself using a lot. I call them *hawk debugging* and *ant debugging*.

The core of debugging is not, as the name might suggest, to remove bugs from your code. It’s about finding them. That’s what my patterns are about.

**Hawk debugging** is for the best case. I use it when I already have a hunch about where the issue could be. Like a hawk on its prey, I swoop down on a specific function or class to check my assumptions. Is some parameter not what it’s supposed to be? Has this function not been called? If it works, I feel like a genius. If it doesn’t, you have to try again. At some point, you might run out of hunches. So, hawk debugging has its limits. It’s fast if it works, but it’s not guaranteed to.

**Ant debugging** to the rescue. Time to switch gears. Ants make progress through persistence and thoroughness, and so do we. We’re not trying to find the problem directly anymore. We’re slowly isolating it. When hawk-debugging, you’re making educated guesses about which parts of the haystack to look at. For ants, it’s all about chipping away at that haystack. Do you think the problem is related to component A? Make sure it is. Take away all other components and try reproducing the issue only with B. This won’t tell you the exact source of the problem, but it’s progress. The problem is related to component A? Dive into it. Which classes and functions inside it can you chop off to still see the issue?


Say you see a crash in your REST API. The consumer sends some payload, and your service returns 500. When you see the stack trace, you immediately go, “Oh damn, yep, okay”. It’s an edge case you hadn’t accounted for. So you come in like a hawk, attach a debugger to what you expect to be the offending line and fire off the request. Sure enough, you find everything as you expect it. You quickly draft a fix PR and send it off for review. Happy end, the hawk saved the day. You’ve reached hacker level 3000. Go ask your boss for a raise.

On the next day, another issue comes around. The machine learning model you have been successfully retraining for ages suddenly gives bad results. (Yes, you’re in machine learning because you hate determinism and happiness.) No problem, unleash the hawk. If the model is bad, perhaps the training loop is faulty. Stoop, nope. Training loop looks good. Perhaps it’s an issue with input feature X? I remember someone changed something about that lately. You check, but it’s a miss. Perhaps the raw data? No again. Slowly, you’re getting worn out and discouraged. In situations like this, you usually think *why the f*** isn’t this working*? It seems like it really should, right? 

No need to despair. Time for ants. Let’s start reducing the size of your haystack. Usually, you would slowly isolate the problem to a specific part of your code. In this case, you could perhaps try finding a faulty part of the pipeline. This will work well if you have intermediate results to compare to. Say you know exactly what the model inputs are supposed to look like. Then, you can compare the status quo to see if everything is fine. In this particular situation, you likely don’t have that. But you have one excellent clue: It used to work before. So, something must have changed. This information allows you to switch to a different debugging dimension: time.

Before, you tried to find *what* breaks your model in the current version of the code. Let’s try finding *when* something changed that broke your model. For this, you need version control, which means git for most people. Prepare a minimal test case to confirm if your model is broken with a current revision. Perhaps you can already tell after one epoch whether that’s the case. So put that in a little script and dispatch my favourite last-resort debugging tool of all time: [git-bisect](https://git-scm.com/docs/git-bisect). Git bisect uses binary search to find out which revision introduced an issue. If you know that build 160 was still good, but 180 is bad, it’ll first test 170. Turns out that one is still good, so the issue must have been introduced later. So we’re testing 175. Two more iterations of this and we find that the issue was introduced in build 174. Now that’s a haystack of manageable size. By looking at the changes in that build, you should be able to figure out what caused this problem.

<!— Add Perceiver anecdote? -->

## Conclusion
- what’s better?
- depends on the situation and your personal style
- I usually go for some stoops of hawk debugging, but when an issue becomes thorny enough, I go for ant debugging
- also for emotional reasons: when hawk debugging becomes exhausting and disheartening, ant debugging can give you reassurance that you’ve not gone mad
- it’s (in principle) guaranteed to be successful
	- That is, barring horrible things like stochasticity, unexpected side-effects, race conditions and other horrible things. But’s that’s a topic for another blog post.